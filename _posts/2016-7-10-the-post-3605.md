---
tags:
  - 'machine learning'
layout: post
catalog: true
title: 'Lecture notes for Stat665'
category: MachineLearning
---

# Lecture notes for Stat665

### Cross-entropy cost function

>  Learning slow is due to the shape of the **sigmoid function**.

When we use sigmoid function, then the partial derivative of the quadratic cost function for the weight and bias are multiplied by $\sigma'(z)$. For example, when the desired output is near 0 while the actual output is 1, then the learning is slow although the neuron's performance is bad.

[![sigmoid](/img/in-post/sigmoid.png "sigmoid")](/img/in-post/sigmoid.png "sigmoid")
As we can see, the curve tends to be flat when Z is bigger.

Thus, we need a better function than the trivial quadratic cost function. And cross-entropy is one of the alternatives. The cost function is 
 $$ C = -\frac{1}{n} \sum_x \left[y \ln a + (1-y ) \ln (1-a) \right],$$
With some algebra calculation, we can prove that the partial derivative of the cost function for weights and bias is independent of $\sigma'(z)$. And it is only related with $ \frac{\partial C}{\partial w_j} =  \frac{1}{n} \sum_x x_j(\sigma(z)-y)$


